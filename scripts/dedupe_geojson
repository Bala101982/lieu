#!/usr/env/bin python

import argparse
import leveldb
import os
import subprocess
import uuid

from six import itertools, operator
from six.moves import xrange

import ujson as json
from collections import Counter, defaultdict

from lieu.address import Address, AddressComponents
from lieu.api import DedupeResponse
from lieu.dedupe import VenueDeduper, AddressDeduper, NameDeduper
from lieu.encoding import safe_encode, safe_decode
from lieu.tfidf import TFIDF
from lieu.input import GeoJSONParser, GeoJSONLineParser

from postal.expand import ADDRESS_NAME

EXACT_DUPE = 'exact_dupe'
LIKELY_DUPE = 'likely_dupe'


def open_geojson_file(filename):
    try:
        f = GeoJSONLineParser(filename)
        f.next_feature()
        f = GeoJSONLineParser(filename)
    except ValueError:
        f = GeoJSONParser(filename)

    return f

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('files', nargs='*')

    parser.add_argument('--address-only',
                        action='store_true',
                        default=False,
                        help='Address duplicates only')

    parser.add_argument('-output-dir', '-o',
                        default='deduped',
                        help='Output directory')

    parser.add_argument('--guids-db-name', '-g',
                        default='guids_db',
                        help='Path to database to store guids')

    parser.add_argument('--tfidf-index', '-d',
                        default='tfidf.index',
                        help='TF-IDF index file')

    parser.add_argument('--temp-filename', '-t',
                        default='near_dupes',
                        help='Temporary sort file')

    parser.add_argument('--output-filename', '-f',
                        default='deduped.geojson',
                        help='Output filename')

    parser.add_argument('--name-dupe-threshold', '-n',
                        type=float,
                        default=DedupeResponse.default_name_dupe_threshold,
                        help='Likely-dupe threshold between 0 and 1 for name deduping with Soft-TFIDF')

    parser.add_argument('--name-review-threshold', '-r',
                        type=float,
                        default=DedupeResponse.default_name_review_threshold,
                        help='Human review threshold between 0 and 1 for name deduping with Soft-TFIDF')

    parser.add_argument('--with-unit',
                        action='store_true',
                        default=False,
                        help='Whether to include units in deduplication')

    args = parser.parse_args()

    address_only = args.address_only
    with_unit = args.with_unit
    name_dupe_threshold = args.name_dupe_threshold
    name_review_threshold = args.name_review_threshold

    tfidf_filename = os.path.join(args.output_dir, args.tfidf_index)

    tfidf_index = None
    if not address_only:
        tfidf_index = TFIDF()

    print('TF-IDF index file: {}'.format(tfidf_filename))

    temp_filename = os.path.join(args.output_dir, args.temp_filename)
    map_file = open(temp_filename, 'w')

    print('Near-dupe tempfile: {}'.format(temp_filename))

    guids_db_path = os.path.join(args.output_dir, args.guids_db_name)
    leveldb.DestroyDB(guids_db_path)

    print('Guids DB: {}'.format(guids_db_path))

    guids_db = leveldb.LevelDB(guids_db_path)

    out_path = os.path.join(args.output_dir, args.output_filename)
    out_file = open(out_path, 'w')

    print('Output filename: {}'.format(out_path))
    print('-----------------------------')

    print('* Assigning IDs, creating near-dupe hashes{}'.format(' + IDF index' if not address_only else ''))

    num_features = 0
    for filename in args.files:
        f = open_geojson_file(filename)

        for i, feature in enumerate(f):
            DedupeResponse.add_random_guid(feature)
            guid = feature['properties'][DedupeResponse.guid_key]
            DedupeResponse.add_guid(feature, guid)
            guids_db.Put(guid, json.dumps(feature))

            address = Address.from_geojson(feature)

            if not address_only:
                name = address.get(AddressComponents.NAME)
                if not name:
                    continue
                tfidf_index.update(Counter(name.lower().split()))
                hashes = VenueDeduper.near_dupe_hashes(address)
            else:
                hashes = AddressDeduper.near_dupe_hashes(address)

            for h in hashes:
                map_file.write(safe_encode(u'{}\t{}\n'.format(h, guid)))

            num_features += 1

    map_file.close()

    if not address_only:
        tfidf_index.save(tfidf_filename)

    print('* Sorting temporary near-dupe file by hash')

    sorted_temp_filename = '{}.sorted'.format(temp_filename)

    subprocess.check_call(['sort', '-t', '\t', '-T', args.output_dir, '-k1,1', '-s', temp_filename, '-o', sorted_temp_filename])

    os.unlink(temp_filename)

    last_key = None
    candidate_dupes = []

    print('* Checking blocks of near-dupe candidates pairwise for dupes')

    dupe_pairs = defaultdict(set)
    dupes = set()
    num_comparisons = 0

    kvs = enumerate(itertools.groupby((safe_decode(line).rstrip().split(u'\t', 1) for line in open(sorted_temp_filename)),
                                      key=operator.itemgetter(0)))

    while True:
        try:
            i, (key, vals) = kvs.next()
            candidate_dupes = [v for k, v in vals]

            num_candidate_dupes = len(candidate_dupes)

            if num_candidate_dupes > 1:
                candidate_features = [(candidate_guid, json.loads(guids_db.Get(candidate_guid))) for candidate_guid in candidate_dupes]

                for ((canonical_guid, canonical_feature), (other_guid, other_feature)) in itertools.combinations(candidate_features, 2):
                    canonical = Address.from_geojson(canonical_feature)
                    other = Address.from_geojson(other_feature)

                    if not address_only:
                        dupe_class, sim = VenueDeduper.dupe_class_and_sim(canonical, other, tfidf=tfidf_index,
                                                                          name_dupe_threshold=name_dupe_threshold,
                                                                          name_review_threshold=name_review_threshold,
                                                                          with_unit=with_unit)

                        if dupe_class is not None:
                            dupe_pairs[other_guid].add((canonical_guid, dupe_class, sim))

                            if dupe_class in (DedupeResponse.classifications.EXACT_DUPE, DedupeResponse.classifications.LIKELY_DUPE):
                                dupes.add(other_guid)
                    elif address_only and AddressDeduper.is_dupe(canonical, other, with_unit=with_unit):
                        dupe_pairs[other_guid].add((canonical_guid, DedupeResponse.classifications.EXACT_DUPE, 1.0))
                        dupes.add(other_guid)

                    num_comparisons += 1

        except StopIteration:
            break

    print('  did {} out of {} possible comparisons'.format(num_comparisons, (num_features * (num_features - 1)) / 2 ))
    os.unlink(sorted_temp_filename)

    print('* Building output file')

    for guid, value in guids_db.RangeIter():
        value = json.loads(value)
        is_dupe = any((classification in (DedupeResponse.classifications.EXACT_DUPE, DedupeResponse.classifications.LIKELY_DUPE) for other_guid, classification, sim in dupe_pairs.get(guid, ())))
        if not is_dupe:
            DedupeResponse.add_guid(value, guid)
        response = DedupeResponse.base_response(value, is_dupe)

        if guid in dupe_pairs:
            for other_guid, classification, sim in dupe_pairs[guid]:
                other_value = json.loads(guids_db.Get(other_guid))

                if not address_only:
                    canonical = Address.from_geojson(value)
                    other = Address.from_geojson(other_value)

                    explain = DedupeResponse.explain_venue_dupe(name_dupe_threshold=name_dupe_threshold, 
                                                                name_review_threshold=name_review_threshold,
                                                                with_unit=with_unit)
                else:
                    explain = DedupeResponse.explain_address_dupe(with_unit=with_unit)

                is_canonical = other_guid not in dupes

                if is_canonical:
                    DedupeResponse.add_guid(other_value, other_guid)
                DedupeResponse.add_possible_dupe(response, value=other_value, classification=classification, is_canonical=is_canonical, similarity=sim, explain=explain)

        out_file.write(json.dumps(response) + '\n')

    out_file.close()
    print('Finished. Got {} dupe records'.format(len(dupes)))
