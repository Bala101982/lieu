#!/usr/env/bin python

import argparse
import leveldb
import os
import subprocess
import uuid

from six import itertools, operator
from six.moves import xrange

import ujson as json
from collections import Counter, defaultdict

from lieu.address import Address, AddressComponents
from lieu.api import DedupeResponse
from lieu.dedupe import VenueDeduper, AddressDeduper, NameDeduper
from lieu.encoding import safe_encode, safe_decode
from lieu.tfidf import TFIDF
from lieu.input import GeoJSONParser, GeoJSONLineParser

from postal.expand import ADDRESS_NAME

EXACT_DUPE = 'exact_dupe'
LIKELY_DUPE = 'likely_dupe'


def open_geojson_file(filename):
    try:
        f = GeoJSONLineParser(filename)
        f.next_feature()
        f = GeoJSONLineParser(filename)
    except ValueError:
        f = GeoJSONParser(filename)

    return f

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('files', nargs='*')

    parser.add_argument('--address-only',
                        action='store_true',
                        default=False,
                        help='Address duplicates only')

    parser.add_argument('-output-dir', '-o',
                        default='deduped',
                        help='Output directory')

    parser.add_argument('--guids-db-name', '-g',
                        default='guids_db',
                        help='Path to database to store guids')

    parser.add_argument('--tfidf-index', '-d',
                        default='tfidf.index',
                        help='TF-IDF index file')

    parser.add_argument('--temp-filename', '-t',
                        default='near_dupes',
                        help='Temporary sort file')

    parser.add_argument('--output-filename', '-f',
                        default='deduped.geojson',
                        help='Output filename')

    parser.add_argument('--name-dupe-threshold', '-n',
                        type=float,
                        default=NameDeduper.default_dupe_threshold,
                        help='Threshold between 0 and 1 for name deduping with Soft-TFIDF')

    parser.add_argument('--with-unit',
                        action='store_true',
                        default=False,
                        help='Whether to include units in deduplication')

    args = parser.parse_args()

    address_only = args.address_only
    with_unit = args.with_unit
    name_dupe_threshold = args.name_dupe_threshold

    tfidf_filename = os.path.join(args.output_dir, args.tfidf_index)

    tfidf_index = None
    if not address_only:
        tfidf_index = TFIDF()

    print('TF-IDF index file: {}'.format(tfidf_filename))

    temp_filename = os.path.join(args.output_dir, args.temp_filename)
    map_file = open(temp_filename, 'w')

    print('Near-dupe tempfile: {}'.format(temp_filename))

    guids_db_path = os.path.join(args.output_dir, args.guids_db_name)
    leveldb.DestroyDB(guids_db_path)

    print('Guids DB: {}'.format(guids_db_path))

    guids_db = leveldb.LevelDB(guids_db_path)

    out_path = os.path.join(args.output_dir, args.output_filename)
    out_file = open(out_path, 'w')

    print('Output filename: {}'.format(out_path))
    print('-----------------------------')

    print('* Assigning IDs, creating near-dupe hashes{}'.format(' + IDF index' if not address_only else ''))

    num_features = 0
    for filename in args.files:
        f = open_geojson_file(filename)

        for i, feature in enumerate(f):
            DedupeResponse.add_random_guid(feature)
            guid = feature['properties'][DedupeResponse.guid_key]
            DedupeResponse.add_guid(feature, guid)
            guids_db.Put(guid, json.dumps(feature))

            address = Address.from_geojson(feature)

            if not address_only:
                name = address.get(AddressComponents.NAME)
                if not name:
                    continue
                tfidf_index.update(Counter(name.lower().split()))
                hashes = VenueDeduper.near_dupe_hashes(address)
            else:
                hashes = AddressDeduper.near_dupe_hashes(address)

            for h in hashes:
                map_file.write(safe_encode(u'{}\t{}\n'.format(h, guid)))

            num_features += 1

    map_file.close()

    if not address_only:
        tfidf_index.save(tfidf_filename)

    print('* Sorting temporary near-dupe file by hash')

    sorted_temp_filename = '{}.sorted'.format(temp_filename)

    subprocess.check_call(['sort', '-t', '\t', '-T', args.output_dir, '-k1,1', '-s', temp_filename, '-o', sorted_temp_filename])

    os.unlink(temp_filename)

    last_key = None
    candidate_dupes = []

    print('* Checking blocks of near-dupe candidates pairwise for dupes')

    dupe_pairs = defaultdict(set)
    num_comparisons = 0

    kvs = enumerate(itertools.groupby((safe_decode(line).rstrip().split(u'\t', 1) for line in open(sorted_temp_filename)),
                                      key=operator.itemgetter(0)))

    while True:
        try:
            i, (key, vals) = kvs.next()
            candidate_dupes = [v for k, v in vals]

            candidate_features = [(candidate_guid, json.loads(guids_db.Get(candidate_guid))) for candidate_guid in candidate_dupes]
            num_candidate_features = len(candidate_features)

            if num_candidate_features > 1:
                for j in xrange(num_candidate_features):
                    canonical_guid, canonical_feature = candidate_features[j]
                    canonical = Address.from_geojson(canonical_feature)

                    for k in xrange(j + 1, num_candidate_features):
                        other_guid, other_feature = candidate_features[k]
                        other = Address.from_geojson(other_feature)

                        if not address_only:
                            venue_dupe = VenueDeduper.is_dupe(canonical, other, tfidf=tfidf_index,
                                                              name_dupe_threshold=name_dupe_threshold, with_unit=with_unit)
                            if venue_dupe:
                                dupe_pairs[other_guid].add(canonical_guid)
                        elif address_only and AddressDeduper.is_dupe(canonical, other, with_unit=with_unit):
                            dupe_pairs[other_guid].add(canonical_guid)

                        num_comparisons += 1
        except StopIteration:
            break

    print('  did {} out of {} possible comparisons'.format(num_comparisons, (num_features * (num_features - 1)) / 2 ))
    os.unlink(sorted_temp_filename)

    print('* Building output file')

    for guid, value in guids_db.RangeIter():
        value = json.loads(value)
        is_dupe = guid in dupe_pairs
        if not is_dupe:
            DedupeResponse.add_guid(value, guid)
        response = DedupeResponse.base_response(value, is_dupe)

        if is_dupe:
            for other_guid in dupe_pairs[guid]:
                other_value = json.loads(guids_db.Get(other_guid))
                exact_match = True
                if not address_only:
                    canonical = Address.from_geojson(value)
                    a1_name = canonical.get(AddressComponents.NAME)
                    other = Address.from_geojson(other_value)
                    a2_name = other.get(AddressComponents.NAME)
                    if a1_name and a2_name:
                        exact_match = VenueDeduper.component_equals(a1_name, a2_name, ADDRESS_NAME, no_whitespace=False)

                    explain = DedupeResponse.explain_venue_dupe(name_dupe_threshold=args.name_dupe_threshold, with_unit=with_unit)
                else:
                    explain = DedupeResponse.explain_address_dupe(with_unit=with_unit)

                is_canonical = other_guid not in dupe_pairs
                classification = EXACT_DUPE if exact_match else LIKELY_DUPE

                if is_canonical:
                    DedupeResponse.add_guid(other_value, other_guid)
                DedupeResponse.add_same_as(response, value=other_value, classification=classification, is_canonical=is_canonical, explain=explain)

        out_file.write(json.dumps(response) + '\n')

    out_file.close()
    print('Finished. Got {} dupe records'.format(len(dupe_pairs)))
    print('Number of canonicals = {}'.format(len(set.union(*dupe_pairs.values()))))
